{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8117590e-17d4-4f8d-a84f-cc751eff0281",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType\n",
    "from pyspark.sql.functions import col, when, count, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "564166c1-4ff4-4181-9304-84ed3da5af3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IPLDataIngestion\") \\\n",
    "    .getOrCreate()\n",
    " \n",
    "string_schema = \"\"\"\n",
    "    match_id INT,\n",
    "    season STRING,\n",
    "    start_date DATE,\n",
    "    venue STRING,\n",
    "    innings INT,\n",
    "    ball FLOAT,\n",
    "    batting_team STRING,\n",
    "    bowling_team STRING,\n",
    "    striker STRING,\n",
    "    non_striker STRING,\n",
    "    bowler STRING,\n",
    "    runs_off_bat INT,\n",
    "    extras INT,\n",
    "    wides INT,\n",
    "    noballs INT,\n",
    "    byes INT,\n",
    "    legbyes INT,\n",
    "    penalty INT,\n",
    "    wicket_type STRING,\n",
    "    player_dismissed STRING,\n",
    "    other_wicket_type STRING,\n",
    "    other_player_dismissed STRING\n",
    "\"\"\"\n",
    "\n",
    "ipl_df_string_schema = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(string_schema) \\\n",
    "    .csv(\"path_to_ipl_data.csv\")  \n",
    "struct_schema = StructType([\n",
    "    StructField(\"match_id\", IntegerType(), True),\n",
    "    StructField(\"season\", StringType(), True),\n",
    "    StructField(\"start_date\", DateType(), True),\n",
    "    StructField(\"venue\", StringType(), True),\n",
    "    StructField(\"innings\", IntegerType(), True),\n",
    "    StructField(\"ball\", FloatType(), True),\n",
    "    StructField(\"batting_team\", StringType(), True),\n",
    "    StructField(\"bowling_team\", StringType(), True),\n",
    "    StructField(\"striker\", StringType(), True),\n",
    "    StructField(\"non_striker\", StringType(), True),\n",
    "    StructField(\"bowler\", StringType(), True),\n",
    "    StructField(\"runs_off_bat\", IntegerType(), True),\n",
    "    StructField(\"extras\", IntegerType(), True),\n",
    "    StructField(\"wides\", IntegerType(), True),\n",
    "    StructField(\"noballs\", IntegerType(), True),\n",
    "    StructField(\"byes\", IntegerType(), True),\n",
    "    StructField(\"legbyes\", IntegerType(), True),\n",
    "    StructField(\"penalty\", IntegerType(), True),\n",
    "    StructField(\"wicket_type\", StringType(), True),\n",
    "    StructField(\"player_dismissed\", StringType(), True),\n",
    "    StructField(\"other_wicket_type\", StringType(), True),\n",
    "    StructField(\"other_player_dismissed\", StringType(), True)\n",
    "])\n",
    "\n",
    "ipl_df_struct_schema = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(struct_schema) \\\n",
    "    .csv(\"path_to_ipl_data.csv\") \n",
    "# Add data quality checks\n",
    "quality_checked_df = ipl_df_struct_schema.withColumn(\"is_valid\", \n",
    "    when(col(\"match_id\").isNull(), lit(False)) \\\n",
    "    .when(col(\"season\").isNull(), lit(False)) \\\n",
    "    .when(col(\"start_date\").isNull(), lit(False)) \\\n",
    "    .when(col(\"innings\").isin(1, 2) == False, lit(False)) \\\n",
    "    .otherwise(lit(True))\n",
    ")\n",
    "\n",
    "valid_records = quality_checked_df.filter(col(\"is_valid\") == True)\n",
    "invalid_records = quality_checked_df.filter(col(\"is_valid\") == False)\n",
    "\n",
    "invalid_records.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "\n",
    "corrected_df = ipl_df_struct_schema.withColumn(\"innings\", \n",
    "    when(col(\"innings\").isin(1, 2), col(\"innings\")) \\\n",
    "    .otherwise(lit(1))  \n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-04-25 09:13:43",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}